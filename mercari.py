# -*- coding: utf-8 -*-
"""Mercari.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16beizEZMWK7xr0E4HJGgMOj8vqMrWGRK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px    #high level interface
import plotly.graph_objects as go   #low level interface  graphical obj/

df = pd.read_csv("/content/playstore_reviews.csv")

df.head()

import warnings
warnings.filterwarnings('ignore')

df.columns

df.duplicated().sum()

df.isnull().sum()

plt.figure(figsize=(15,6))
sns.countplot(x=df['score'], data = df, palette = 'hls')
plt.show()

df['at'] = pd.to_datetime(df['at'])

df['at']

df1 = df.copy()

df1.set_index('at',inplace=True)

start_date = df1.index.min()
end_date = df1.index.max()

print("start date: ",start_date)
print("end date: ",end_date)

week_delta = pd.Timedelta(days=6)
current_date = start_date

while current_date<=end_date:
  start_week = current_date
  print(start_week)
  end_week = current_date + week_delta
  print(end_week)
  current_week_data = df1[(df1.index >= start_week) & (df1.index < end_week)]
  print(current_week_data)
  weekly_counts = current_week_data.resample('D').size()   #stats of a particular day {D}
  print(weekly_counts)
  fig = px.bar(weekly_counts, x=weekly_counts.index, y=weekly_counts.values,
                 labels={'x': 'Date', 'y': 'Number of Reviews'},
                 title=f'Reviews for Week {start_week.strftime("%Y-%m-%d")} to {end_week.strftime("%Y-%m-%d")}')
  fig.update_layout(xaxis_tickangle=-45)
  fig.show()
  current_date += week_delta

daily_counts = df1.resample('D').size()

fig = go.Figure()
fig.add_trace(go.Bar(x=daily_counts.index, y=daily_counts.values,
                     marker_color='skyblue'))
fig.update_layout(title='Number of Reviews Day-wise',
                  xaxis_title='Date',
                  yaxis_title='Number of Reviews',
                  xaxis_tickangle=-45)
fig.show()

fig = go.Figure()
for source_name, source_data in df1.groupby('score'):
    fig.add_trace(go.Scatter(x=source_data.resample('D').size().index, y=source_data.resample('D').size().values,
                            mode='lines', name=source_name))
fig.update_layout(title='Number of Reviews Day-wise',
                  xaxis_title='Date',
                  yaxis_title='Number of Reviews',
                  xaxis_tickangle=-45)
fig.show()

fig = go.Figure()
for rating_val, rating_data in df1.groupby('score'):
    fig.add_trace(go.Scatter(x=rating_data.resample('D').size().index, y=rating_data.resample('D').size().values,
                             mode='lines+markers', name=f'Rating {rating_val}'))
fig.update_layout(title='Number of Reviews Day-wise by Rating',
                  xaxis_title='Date',
                  yaxis_title='Number of Reviews',
                  xaxis_tickangle=-45)
fig.show()

df['score']=df['score'].map({1:-1,2:-1,3:0,4:1,5:1})

df

df_new = df[['content', 'score']]

df_new['score'].unique()

fig = go.Figure(data=[go.Bar(x=df_new['score'].value_counts().index, y=df_new['score'].value_counts())])
fig.update_layout(title='Rating',xaxis_title="Rating",yaxis_title="Count")
fig.show()

counts = df_new['score'].value_counts()
fig = go.Figure(data=[go.Pie(labels=counts.index, values=counts)])
fig.update_layout(title='Rating')
fig.show()

"""#NLP"""

def clean_text(text):
  text = text.lower()
  return text.strip()

df_new.content = df_new.content.apply(lambda x: clean_text(x))

df_new.content

import string
string.punctuation

def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree

df_new.content = df_new.content.apply(lambda x:remove_punctuation(x))

df_new.content = df_new.content.apply(lambda x: x.lower())

import re

def tokenization(text):
    tokens = re.split(' ',text)
    return tokens

df_new.content = df_new.content.apply(lambda x: tokenization(x))

df_new.content

import nltk
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')
stopwords[0:10]
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]

def remove_stopwords(text):
    output= " ".join(i for i in text if i not in stopwords)
    return output

df_new.content = df_new.content.apply(lambda x:remove_stopwords(x))

df_new.content

def clean_text(text):
    text = re.sub('\[.*\]','', text).strip()
    text = re.sub('\S*\d\S*\s*','', text).strip()
    return text.strip()

df_new.content = df_new.content.apply(lambda x: clean_text(x))

df_new.content

import spacy
nlp = spacy.load('en_core_web_sm')

stopwords = nlp.Defaults.stop_words
def lemmatizer(text):
    doc = nlp(text)
    sent = [token.lemma_ for token in doc if not token.text in set(stopwords)]
    return ' '.join(sent)
  #stemmimg will give root word but not necesserily meaningful
  #but lemmetization will give root word which is meaningful

df_new.content =  df_new.content.apply(lambda x: lemmatizer(x))

df_new.content

def remove_urls(vTEXT):
    vTEXT = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', vTEXT, flags=re.MULTILINE)
    return(vTEXT)

df_new.content = df_new.content.apply(lambda x: remove_urls(x))

def remove_digits(text):
    clean_text = re.sub(r"\b[0-9]+\b\s*", "", text)
    return(text)

df_new.content = df_new.content.apply(lambda x: remove_digits(x))

def remove_digits1(sample_text):
    clean_text = " ".join([w for w in sample_text.split() if not w.isdigit()])
    return(clean_text)

df_new.content= df_new.content.apply(lambda x: remove_digits1(x))

def remove_emojis(data):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"
                               u"\U0001F300-\U0001F5FF"
                               u"\U0001F680-\U0001F6FF"
                               u"\U0001F1E0-\U0001F1FF"
                               "]+", flags=re.UNICODE)
    return re.sub(emoji_pattern, '', data)

df_new

import wordcloud

from wordcloud import WordCloud
data = df_new.content
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
               collocations=False).generate(" ".join(data))
plt.imshow(wc)
plt.axis('off')
plt.show()

from wordcloud import WordCloud
data = df_new[df_new['score'] == 1]['content']
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
               collocations=False).generate(" ".join(data))
plt.imshow(wc)
plt.axis('off')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer

tf1=TfidfVectorizer()
data_vec=tf1.fit_transform(df_new['content'])

data_vec

import pickle

with open ('tfidf_vectorizer.pkl','wb') as model_file:
    pickle.dump(tf1, model_file)   #in a dump file the contents can not be understood, serilisation deserialisation of the dtata

y=df_new['score'].values

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(data_vec,y,test_size=0.2,stratify = y, random_state=42)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X_train, y_train)
#if two classes data are not in equal proportions or quantities then we use smote to balance thheir number of imgs/workds

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

sv = SVC()
dt = DecisionTreeClassifier()
rf = RandomForestClassifier()
ad = AdaBoostClassifier()

models = [sv, dt, rf, ad]

accuracies = []

for model in models:
    print('Results for the model:', model.__class__.__name__)
    model.fit(X_balanced, y_balanced)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print('Accuracy:', accuracy)

    cm = confusion_matrix(y_test, y_pred)
    print('Confusion Matrix:\n', cm)

    report = classification_report(y_test, y_pred)
    print('Classification Report:\n', report)

    print('\n')

    accuracies.append(accuracy)

print('List of Accuracies:', accuracies)

import pickle

sv=SVC()
model_sv =  sv.fit(X_balanced, y_balanced)
model_filename = 'svm_model.pkl'
with open(model_filename, 'wb') as model_file:
    pickle.dump(model_sv,model_file)

model_names = ['SVC', 'DecisionTree', 'RandomForest', 'AdaBoost']
fig = go.Figure(data=go.Bar(x=model_names, y=accuracies))
fig.update_layout(title='Model Accuracies',
                  xaxis_title='Model',
                  yaxis_title='Accuracy',
                  yaxis_tickformat='.2%',
                  yaxis_range=[0, 1],
                  xaxis_tickangle=0)
fig.show()

model_filename = 'svm_model.pkl'
with open(model_filename,'wb') as model_file:
  pickle.dump(model_sv,model_file)

import streamlit as st
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from PIL import Image
import re
import string
import nltk
import spacy
import sklearn
import joblib

# Check the version of Streamlit
print(f"Streamlit version: {st.__version__}")

# Check the version of PIL (Pillow)
pillow_version = Image.__version__
print(f"Pillow version: {pillow_version}")

# Check the version of re (regular expressions)
print(f"Python re (regular expressions) version: {re.__version__}")

# Check the version of NLTK (Natural Language Toolkit)
nltk_version = nltk.__version__
print(f"NLTK version: {nltk_version}")

# Check the version of spaCy
spacy_version = spacy.__version__
print(f"spaCy version: {spacy_version}")

# Check the version of scikit-learn
sklearn_version = sklearn.__version__
print(f"scikit-learn version: {sklearn_version}")

# Check the version of joblib
joblib_version = joblib.__version__
print(f"joblib version: {joblib_version}")
